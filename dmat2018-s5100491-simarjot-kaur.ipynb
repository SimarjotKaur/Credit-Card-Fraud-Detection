{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import sqrt\n\nimport sklearn.utils\nfrom sklearn.preprocessing import RobustScaler #Scaling the features\nfrom sklearn.model_selection import StratifiedShuffleSplit #Splitting the dataset\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, auc\nfrom sklearn.model_selection import GridSearchCV #hyperparameter tuning\nfrom sklearn.decomposition import PCA\n\n#Visual Analysis\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nimport IPython\n\nfrom imblearn.over_sampling import ADASYN #Adaptive Synthetic Oversampling\nfrom collections import Counter\nfrom scipy import stats\n\n#Neural Networks implementation\nimport keras\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\nfrom keras.utils.vis_utils import plot_model\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"688cfbf93de7e58bc5812f0fd4269d3214111dd5"},"cell_type":"markdown","source":"#DATASET ANALYSIS AND PREPROCESSING"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Loading data in a dataframe 'creditcard_data' and using head() to display the first 5 instances.\ncreditcard_data= pd.read_csv(\"../input/creditcard.csv\")\ncreditcard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8839a473606eb81d88eb141e7d8b6a1ff9f51e4d"},"cell_type":"code","source":"#Use parameter 'n' to display instances other than 5.\ncreditcard_data.head(n=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"348fb240bbcc1ad3c9ccf7e32168d4a94d739ec7"},"cell_type":"code","source":"#Number of instances and attributes,i.e., Dimensionality of the dataset\ncreditcard_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a800376a978651bf103a947b5f33835602645f2a"},"cell_type":"code","source":"creditcard_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d4dbb7f5842d34dbed7f4fe764eb31f56da666"},"cell_type":"markdown","source":"This shows that there are 284807 instances and 31 attributes including the class attribute."},{"metadata":{"trusted":true,"_uuid":"e0c2a771f98176ced86437b5ed5c7d9c19c2996b"},"cell_type":"code","source":"#Sum of missing cells for each attribute\ncreditcard_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2ba33fa4673e5531714e5aef568e3ab12d7c737"},"cell_type":"markdown","source":"The 0 sum for all attributes shows that there are no missing values."},{"metadata":{"trusted":true,"_uuid":"20cffdc3ddfcfed6b39cbc5be053b02fbcdde52f"},"cell_type":"code","source":"#Number of distinct categories or classes i.e., Fraudulent and Genuine\ncreditcard_data['Class'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ab705a42d3e999d07c2c025161bb3a4f3df05d"},"cell_type":"markdown","source":"As expected, there are only 2 classes."},{"metadata":{"trusted":true,"_uuid":"65ee58a18183a27bc5ee02e0ab7a3bce0fe2b2ae"},"cell_type":"code","source":"#number of instances per class\ncreditcard_data.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c54353c3288e571a20029b7acd6e54656068cb2"},"cell_type":"markdown","source":"This shows a complete imbalance of classes. There are 284315 'Genuine' (0) instances and only 492 'Fraudulent' (1) instances."},{"metadata":{"trusted":true,"_uuid":"6a4fc54b30e26cace74115aaf0df177b2f24e764"},"cell_type":"code","source":"#visual representation of instances per class\ncreditcard_data.Class.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"944cffb442226b9e727c753cdd01a6fc0f33a9a6"},"cell_type":"markdown","source":"This gives a visual representation of the class imbalance. The below plot after PCA gives a better visualization of the imbalnce in the datasets."},{"metadata":{"trusted":true,"_uuid":"fd5ecfc1499af89889e780e98a2ac702b3faeced","_kg_hide-output":false},"cell_type":"code","source":"#Before sampling (PCA is performed for visualization only)\npca= PCA(n_components=2)\ncreditcard_2d= pd.DataFrame(pca.fit_transform(creditcard_data.iloc[:,0:30]))\ncreditcard_2d= pd.concat([creditcard_2d, creditcard_data['Class']], axis=1)\ncreditcard_2d.columns= ['x', 'y', 'Class']\nsns.lmplot(x='x', y='y', data=creditcard_2d, fit_reg=False, hue='Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5f6d97f286ff55676fae9530c2899b9accd7906"},"cell_type":"code","source":"#Descriptive Statistics\ncreditcard_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262cd8032c3c0fc673b24e1a736879f49c85fe8b"},"cell_type":"code","source":"#checking the percentage of each class in the dataset\n(creditcard_data.Class.value_counts())/(creditcard_data.Class.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6582d8d6d1de0622a44270d381538362ee898ae4"},"cell_type":"code","source":"#Histrogram for feature Time\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nax1.hist(creditcard_data[\"Time\"][creditcard_data[\"Class\"] == 1], bins = 50)\nax1.set_title('Fraudulent')\n\nax2.hist(creditcard_data[\"Time\"][creditcard_data[\"Class\"] == 0], bins = 50)\nax2.set_title('Genuine')\n\nplt.xlabel('Seconds after transaction number zero')\nplt.ylabel('Number of Transactions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd89458d4088fbcdd4691924d6b50c30e5e4b5d3"},"cell_type":"markdown","source":"The transactions occur in a cyclic way. But the time feature does not provide any useful information as the time when the first transaction was initiated is not given. Thus, we'll drop this feature."},{"metadata":{"trusted":true,"_uuid":"6109f88ba2d24897a53432f79e7061d7b31c1970"},"cell_type":"code","source":"#dropping Time because it does not give any valuable information\ncreditcard_data = creditcard_data.drop(\"Time\", axis = 1)\ncreditcard_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c77f25ba099bc6a1f04b20a2710d86f274fb010"},"cell_type":"markdown","source":"Now there are 30 features in the dataset."},{"metadata":{"trusted":true,"_uuid":"62ead20165ec5089338dee6b74e5a794ce66239c"},"cell_type":"code","source":"#Descriptive statistics for Fraudulent Transactions\nprint(\"Fraudulent Transactions\")\ncreditcard_data['Amount'][creditcard_data['Class']==1]. describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96dd8124796b4617acf013c353b3dea1ec5c42c9"},"cell_type":"code","source":"#Descriptive statistics for Genuine Transactions\nprint(\"Genuine Transactions\")\ncreditcard_data['Amount'][creditcard_data['Class']==0]. describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00320434cf9b59ccc0f14b7dd3106704451286b6"},"cell_type":"markdown","source":"Nothing much can be determined from the Amount, as most of the transactions are around 100 in both cases.."},{"metadata":{"trusted":true,"_uuid":"ceca6db901ec119d7f66fe7e9855dc246744d91a"},"cell_type":"code","source":"#Variance\ncreditcard_data.var()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"505097712d8283e6f5cba1736b2246f16c57194c"},"cell_type":"markdown","source":"Amount needs to be scaled. As if a feature has a variance orders of magnitude larger than the rest of the features, then it might dominate and, make the estimator unable to learn from other features as expected."},{"metadata":{"trusted":true,"_uuid":"18ed63266e1ec2c3a66faece83561da2062e2942"},"cell_type":"code","source":"#Boxplot for the Amount feature, in order to visualiza the outliers.\nsns.boxplot(x=creditcard_data['Class'], y=creditcard_data['Amount'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f125499cc109bc18c2c9973f7ff49a3e8a8cdb59"},"cell_type":"markdown","source":"The non-fraudulent instances are highly skewed. The outliers can be seen visually and thus, it's better to use a Robust Scaler to scale the feature 'Amount' as it is less prone to outliers."},{"metadata":{"trusted":true,"_uuid":"c9e282023e018c984bfb33bc46662253b23cf68d"},"cell_type":"code","source":"#Standardizing the Amount column (All other 'V' columns are already scaled as they've undergone PCA transformation).\nRob_scaler=RobustScaler() #Robust to outliers\ncreditcard_data['Std_Amount'] = Rob_scaler.fit_transform(creditcard_data['Amount'].values.reshape(-1, 1))\ncreditcard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48607508d0c739386e7218bd0b3c02d335480296"},"cell_type":"code","source":"#drop Amount and move Std_Amount at index '0'\ncreditcard_data = creditcard_data.drop('Amount',axis=1)\n\nStd_Amount = creditcard_data['Std_Amount']\ncreditcard_data.drop('Std_Amount', axis=1, inplace=True)\ncreditcard_data.insert(0, 'Std_Amount', Std_Amount)\ncreditcard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eea30ac82c117bc52729b0b105012810138140d7"},"cell_type":"code","source":"#Splitting data before sampling\n#Splitting data into train and test set in 80% and 20% respectively, using Stratified Shuffle Split\n\nX = creditcard_data.drop('Class', axis=1)\nY = creditcard_data['Class']\n\nsss1 = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\nfor train_index1, test_index in sss1.split(X, Y):\n    print(\"Train:\", train_index1, \"Test:\", test_index)\n    Xtrain, Xtest = X.iloc[train_index1], X.iloc[test_index]\n    Ytrain, Ytest = Y.iloc[train_index1], Y.iloc[test_index]\n\n#Splitting the train set further into train and validation set, which leaves train set 60% of the originial dataset and, test and validation sets 20% each respectively.\nsss2 = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=42)\n\nfor train_index2, val_index in sss2.split(Xtrain, Ytrain):\n    print(\"Train:\", train_index2, \"Validation:\", val_index)\n    Xtrain_final, Xval = X.iloc[train_index2], X.iloc[val_index]\n    Ytrain_final, Yval = Y.iloc[train_index2], Y.iloc[val_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb7bdeffb64b5e0e6adfd1f604e5918d06d3ce81"},"cell_type":"markdown","source":"Datasets created after splitting are:\n1. Train Dataset: Xtrain_final, Ytrain_final\n2. Test Dataset: Xtest, Ytest\n3. Validation Dataset: Xval, Yval"},{"metadata":{"trusted":true,"_uuid":"01b86e5a7fef8faae526022b6c4879f2b09dd48e"},"cell_type":"code","source":"# Check if the labels are distributed equally in all the datasets after splitting\ntrain_unique_label, train_counts_label = np.unique(Ytrain_final, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(Ytest, return_counts=True)\nval_unique_label, val_counts_label = np.unique(Yval, return_counts=True)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(Ytrain_final))\nprint(test_counts_label/ len(Ytest))\nprint(val_counts_label/ len(Yval))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"990cf759e3148bd31dd8b73b42b0755915756cc8"},"cell_type":"markdown","source":"As seen above, the labels are almost equally distributed in all the three datasets"},{"metadata":{"trusted":true,"_uuid":"59d177ae8b5f1b1223f7bbe9bdb7e7acd3dd15b1"},"cell_type":"code","source":"#Dimensionality of the datasets retrieved after splitting\nprint(Xtrain_final.shape)\nprint(Ytrain_final.shape)\nprint(Xtest.shape)\nprint(Ytest.shape)\nprint(Xval.shape)\nprint(Yval.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b84a50d6d3a0bdb790eb1b9f3e17fdc36576d653"},"cell_type":"code","source":"#checking correlation between features and the likelihood of the transaction to be fraud on the unbalanced dataset\nf, ax1 = plt.subplots(figsize=(24,10))\n\ncorr = creditcard_data.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"935172cb925d7b448e2aaba82c8ccd21086a7d95"},"cell_type":"markdown","source":"As seen, the correlations cannot be properly visualized because of the imbalance in the dataset. This is because the correlation matrix is affected by the high imbalance betwen the classes. So, lets balance our classes and then visualize the correlation matrix again."},{"metadata":{"trusted":true,"_uuid":"6eca21f4538c2177c3f9d63d0fd2a16a4cb1d450"},"cell_type":"code","source":"#Checking number of instances for each class in the train dataset\nprint(Ytrain_final.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"241ccb99a6217db12d4d2559facfca900dea11a3"},"cell_type":"code","source":"#Using ADASYN for Oversampling\nada = ADASYN(sampling_strategy='minority', random_state=42)\n\n#Oversampling is applied only on the training set\nX_adasampled, Y_adasampled = ada.fit_sample(Xtrain_final, Ytrain_final)\nprint('Resampled dataset shape %s' % Counter(Y_adasampled))\nprint('Shape of X_adasampled: {}'.format(X_adasampled.shape))\nprint('Shape of Y_adasampled: {}'.format(Y_adasampled.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20821c5430aa0784c19135fc105f59999beb14cb"},"cell_type":"code","source":"#check the disribution of both the labels\ntrain_label, train_count = np.unique(Y_adasampled, return_counts=True)\nprint('Label Distributions: \\n')\nprint(train_count/ len(Y_adasampled))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bf9319b818ef709b3c9123ee24c778d04101cc1"},"cell_type":"code","source":"print(type(X_adasampled))\nprint(type(Y_adasampled))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a703a77685eeb5fff3ac3c1fb4531c8ca875ce60"},"cell_type":"markdown","source":" X_adasampled and Y_adasampled are the balanced train sets resulted after using OverSampling. These are ndarrays.\nIn order to visualize a correlation matrix, they are converted to a dataframe and joined to form a single dataframe in the below series of blocks."},{"metadata":{"trusted":true,"_uuid":"0f1b4289d662d64308cf7ccc326d6e16d4a647ee"},"cell_type":"code","source":"Xsm_train_df = pd.DataFrame(X_adasampled)\nXsm_train_df.columns = Xtrain.columns\nYsm_train_df = pd.DataFrame(Y_adasampled)\nYsm_train_df = Ysm_train_df.rename(columns={0: 'Class'})\nprint(Xsm_train_df.head())\nprint(Ysm_train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d3aa02c156e3590993063fa3221a7222e522d5"},"cell_type":"code","source":"#Merging the Xsm_train_df and Ysm_train_df based on the index values to get a single dataframe in order to visualize a correlation matrix\nnew_df= pd.merge(Xsm_train_df, Ysm_train_df, how='inner', left_index=True, right_index=True)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e425631a8a1db0a547dd702da48329b322eaf19"},"cell_type":"markdown","source":"Hence, the data frames Xsm_train_df and Ysm_train_df are merged to form a new dataframe 'new_df', with 30 attributes in total comprising of 'Std_Amount', 'V1-V28' from Xsm_train_df and 'Class' from Ysm_train_df."},{"metadata":{"trusted":true,"_uuid":"631e5a64a6574d6b5236c2f160825f539fe8e945"},"cell_type":"code","source":"#checking correlation between features on the balanced dataset\nf, ax1 = plt.subplots(figsize=(24,10))\n\ncorr = new_df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Balanced Correlation Matrix\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b3b40ba9e7609d3ef948412cf77a9291e2ce87"},"cell_type":"markdown","source":"Negative and Positive Correlations can be seen in the matrix above:\n* Features, V14, V12, V10 and V3 show negative correlation towards the 'Class', As, lower are these values, more likely the transaction is Fraud.\n* Features V4, V11, V2 and V19 show positive correlation. As, higher are these values, more likely the transaction is Fraud.\n"},{"metadata":{"trusted":true,"_uuid":"cfea01724e1dd53f693a5144d66e664059d43cbc"},"cell_type":"code","source":"#visualizing balanced train dataset\nX_df= pd.DataFrame(X_adasampled)\nY_df= pd.DataFrame(Y_adasampled)\nY_df=Y_df.rename(columns={0: 'Class'})\ntrain_2d= pd.DataFrame(pca.fit_transform(X_df.iloc[:,0:29]))\ntrain_2d= pd.concat([train_2d, Y_df], axis=1)\ntrain_2d.columns= ['x', 'y', 'Class']\nsns.lmplot(x='x', y='y', data=train_2d, fit_reg=False, hue='Class')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"833f2a8b50f2df660a0c36bdaff837ea46cf2e11"},"cell_type":"markdown","source":"The above plot gives a visual representation of the balanced dataset. As seen, the data is not linearly separable. Hence, a Multi-Layer Perceptron is trained on this data, in order to predict the data as 'fraud' or 'genuine'.\nFor the training of the Multi-layer perceptron, many parameters need to be tuned, thus a Grid-Search is used for this purpose. The Grid-Search is performed on the subset of the data, as it takes a lot of time to run. The following blocks create a subset of the dataset using random undersampling used only for this purpose."},{"metadata":{"trusted":true,"_uuid":"e3ed1cb24a125bd09d1875d11e6e33e5a63f658a"},"cell_type":"code","source":"#An undersampled dataset is created to tune the parameters using Grid-Search\nYtrain_df = pd.DataFrame(Ytrain_final)\nYtrain_df = Ytrain_df.rename(columns={0: 'Class'})\nTrain_set= pd.merge(Xtrain_final, Ytrain_df, how='inner', left_index=True, right_index=True)\nTrain_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29ba06c4daccdf57de0450a0ed0f543851cc150c"},"cell_type":"code","source":"Train_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc857b2340fb2e1918d7e6625f5330f347778848"},"cell_type":"code","source":"#Length of Frauds\nlength_frauds=len(Train_set[Train_set.Class==1])\nlength_frauds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af6bd443f22d0e83c16b0c834883b3fe9321fc4"},"cell_type":"code","source":"#Taking only Frauds in a dataframe\nfraud_df= Train_set.loc[Train_set['Class'] == 1]\nfraud_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92a3514716361fb67e78ff83444c484fa36b4311"},"cell_type":"code","source":"#Taking only Non-frauds\nnon_fraud_df= Train_set.loc[Train_set['Class'] == 0]\n#Randomly select the same number of records as the Frauds\nnon_frauds=non_fraud_df.sample(n=length_frauds)\nnon_frauds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef4832d5ac747417dc6a0e9a959ca568375c2a6"},"cell_type":"code","source":"#Appending both Frauds and Non-Frauds\nundersampled_df= fraud_df.append(non_frauds)\n#Randomly shuffling all the instances\nundersampled_df = sklearn.utils.shuffle(undersampled_df)\nundersampled_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"344766cba101a31baccc98047d26b2e8a4a886e6"},"cell_type":"code","source":"#Separating labels from the other features\nX_undersample = undersampled_df.drop('Class', axis=1)\nY_undersample = undersampled_df['Class']\nprint(X_undersample.shape)\nprint(Y_undersample.shape)\nX_undersample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7f88bea27917e2d59d4eb62dea5cef71eab8cf"},"cell_type":"code","source":"#Converting undersampled train sets in numpy arrays\nX_us_arr= X_undersample.values\nY_us_arr= Y_undersample.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02a0202451ec896fc21bb1163e152447fd97faa6"},"cell_type":"code","source":"#Converting test and validation dataframes in numpy arrays\nXval_arr=Xval.values\nYval_arr=Yval.values\nXtest_arr=Xtest.values\nYtest_arr=Ytest.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9ea2c6cccab5dabbf67eab192d05272906a02f1"},"cell_type":"markdown","source":"The code in the following block is used for hyperparameter tuning of different parameters required for the training of a multilayer perceptron. It is commented as it takes alot of time to run. It is run on the undersampled dataset created above. Moreover, the biggest challenge is reproducability. Even after using the seed, the outputs produced vary a bit. But after multiple runs, the parameters that were returned were:\nneuron_1=65, \ndropout_rate=0.5, \ninit_mode='he_normal', \nlearn_rate=0.001"},{"metadata":{"trusted":true,"_uuid":"b93a29e0631ec91dd9f403c8d78a54a53b3a498d"},"cell_type":"code","source":"# #Grid Search for hyperparameter tuning\n# def create_model(neuron_1=29, dropout_rate=0.0, init_mode='uniform', learn_rate=0.01):\n#     n_inputs = X_undersample.shape[1]\n#     model = Sequential()\n#     model.add(Dense(neuron_1, input_shape=(n_inputs, ), kernel_initializer=init_mode, activation= 'relu'))\n#     model.add(Dropout(dropout_rate))\n#     model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n#     model.compile(Adam(lr=learn_rate), loss='binary_crossentropy')\n#     return model\n# #Keras models can be used with scikit learn by wrapping them with KerasClassifier\n# model_tuning = KerasClassifier(build_fn=create_model,epochs=30, batch_size=700, verbose=1)\n\n# neuron_1= [29, 30, 35, 40, 45, 50,55,60,65,70]\n# learn_rate= [0.001, 0.01, 0.1, 0.2, 0.3]\n# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n# init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n# param_grid = dict(neuron_1=neuron_1, dropout_rate=dropout_rate, init_mode=init_mode, learn_rate=learn_rate)\n# grid = GridSearchCV(estimator=model_tuning, param_grid=param_grid, scoring=make_scorer(f1_score), cv=5, n_jobs=-1)\n# grid_result = grid.fit(X_us_arr, Y_us_arr)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# mean_score = grid_result.cv_results_['mean_test_score']\n# std_score = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(mean_score, std_score, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8cfdd97a43ffb5ec96ccc35567eea02a16fac8e"},"cell_type":"markdown","source":"#EXPERIMENTAL DESIGN"},{"metadata":{"_uuid":"ff1a96a0081a8071333c5d21aad394a8744f46e9"},"cell_type":"markdown","source":"Hypothesis: A neural network’s performance on the Credit-Card Fraud dataset is affected by the number of hidden layers.\n* Null Hypothesis (H0): Insufficient evidence to support hypothesis\n* Alternate Hypothesis (H1): Evidence suggests the hypothesis is likely true."},{"metadata":{"_uuid":"a598ad71140231b27c3f4edb94ac19655a1105fb"},"cell_type":"markdown","source":"In this notebook, 2 models are trained:\n1. Multi-layer Perceptron with 1 hidden layer and,\n2. Multi-layer Perceptron with 2 hidden layers\n1. Rest all parameters are kept the same in both the models."},{"metadata":{"_uuid":"c909e158227780ac3418e192192f65b606eba96f"},"cell_type":"markdown","source":"Since, Neural Networks are stochastic and output different results for each run with the same data. The model will be run for a specific number of iterations so that the average score of all the repetitions can determine the skill of the model. Thus, now this sample size sufficiency needs to be determined. For this purpose the multi layer perceptron  model is trained for 100 (selected randomly) iterations and F-score is returned for all the repetitions."},{"metadata":{"_uuid":"1f4cb3b24f844939d7972776ed1a8b0571458c1c"},"cell_type":"markdown","source":"The iterations below were run to determine the sufficient sample size required. The output returned is:\nf_scorelist1= [0.7627118644067796, 0.749003984063745, 0.7419354838709676, 0.7457627118644068, 0.7018867924528301, 0.7294117647058823, 0.6643109540636042, 0.753968253968254, 0.7018867924528301, 0.6888888888888889, 0.6478873239436619, 0.7430830039525692, 0.7410358565737051, 0.7479674796747967, 0.7175572519083968, 0.7634854771784232, 0.7265625, 0.6690647482014389, 0.7159533073929961, 0.7258064516129031, 0.7540983606557377, 0.7315175097276265, 0.7045454545454546, 0.7580645161290323, 0.7076923076923076, 0.6816479400749064, 0.7407407407407407, 0.7041198501872659, 0.7209302325581397, 0.7171314741035857, 0.6789667896678966, 0.7272727272727273, 0.732283464566929, 0.7583333333333334, 0.73015873015873, 0.7230769230769232, 0.7531380753138075, 0.7459016393442623, 0.6893939393939396, 0.7622950819672131, 0.7295081967213115, 0.7450980392156862, 0.7782426778242677, 0.6992481203007519, 0.748971193415638, 0.7126436781609196, 0.7364341085271319, 0.6842105263157894, 0.7317073170731707, 0.7848101265822784, 0.7104247104247104, 0.6666666666666666, 0.7591836734693878, 0.7401574803149606, 0.6946564885496183, 0.7272727272727273, 0.7175572519083968, 0.6946564885496183, 0.6940298507462687, 0.7109375, 0.6966292134831462, 0.7510204081632654, 0.7603305785123966, 0.6713780918727914, 0.7460317460317459, 0.7398373983739839, 0.7449392712550608, 0.673913043478261, 0.6881720430107526, 0.7551867219917012, 0.7215686274509804, 0.6789667896678966, 0.6966292134831462, 0.7551867219917012, 0.7666666666666667, 0.7215686274509804, 0.732283464566929, 0.732283464566929, 0.7181467181467182, 0.7250996015936256, 0.6791044776119403, 0.7109375, 0.7203065134099617, 0.7410358565737051, 0.7603305785123966, 0.681159420289855, 0.7551867219917012, 0.7368421052631579, 0.7142857142857143, 0.6416382252559727, 0.7294117647058823, 0.7531380753138075, 0.7698744769874477, 0.7131782945736435, 0.775, 0.5987261146496816, 0.7099236641221374, 0.7265625, 0.7372549019607844, 0.7280000000000001]"},{"metadata":{"_uuid":"cb07eaeded13204f779195100b5ce312e1271ecd"},"cell_type":"markdown","source":"* The following code is commented as it was run for 100 iterations to determine the sufficient sample size, and it takes alot of time to run. The results are shown above and plotted in the following blocks."},{"metadata":{"trusted":true,"_uuid":"87f50b2d631c7ca2e2c1deb10b0a344ac6eddf86"},"cell_type":"code","source":"# f_scorelist1=[]\n\n# n_inputs = X_adasampled.shape[1]\n# es= keras.callbacks.EarlyStopping(monitor='val_loss',\n#                               min_delta=0,\n#                               patience=2,\n#                               verbose=0, mode='min', restore_best_weights= True)\n# for i in range(0,100):\n#     First_Model = Sequential()\n#     First_Model.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\n#     First_Model.add(Dropout(0.5))\n#     First_Model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\n#     First_Model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n#     First_Model.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=0)\n#     Y_first_pred = First_Model.predict_classes(Xtest_arr, batch_size=200, verbose=0)\n    \n#     f_score= f1_score(Ytest_arr,Y_first_pred)\n#     f_scorelist1.append(f_score)\n\n# print(f_scorelist1)\n# print('FScore Mean: '+ str(np.mean(f_scorelist1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed6242137c57e6cbef4829f9d744e2142c33928f"},"cell_type":"markdown","source":"Now, the number of iterations are plotted against the average score from those iterations to determine the number of iterations at which the mean of the F-score gets stabilized.\nThese number of iterations will be considered as the sample size sufficiency, for which our model will be iterated."},{"metadata":{"trusted":true,"_uuid":"ff459a2321e9dab2c378226dad22b75b80b11f7e"},"cell_type":"code","source":"#Plotting number of repeats against the average score from those repeats.\n#Expected: As the number of repeats increase, the average score stabilizes\nf_score_list=[0.7627118644067796, 0.749003984063745, 0.7419354838709676, 0.7457627118644068, 0.7018867924528301, 0.7294117647058823, 0.6643109540636042, 0.753968253968254, 0.7018867924528301, 0.6888888888888889, 0.6478873239436619, 0.7430830039525692, 0.7410358565737051, 0.7479674796747967, 0.7175572519083968, 0.7634854771784232, 0.7265625, 0.6690647482014389, 0.7159533073929961, 0.7258064516129031, 0.7540983606557377, 0.7315175097276265, 0.7045454545454546, 0.7580645161290323, 0.7076923076923076, 0.6816479400749064, 0.7407407407407407, 0.7041198501872659, 0.7209302325581397, 0.7171314741035857, 0.6789667896678966, 0.7272727272727273, 0.732283464566929, 0.7583333333333334, 0.73015873015873, 0.7230769230769232, 0.7531380753138075, 0.7459016393442623, 0.6893939393939396, 0.7622950819672131, 0.7295081967213115, 0.7450980392156862, 0.7782426778242677, 0.6992481203007519, 0.748971193415638, 0.7126436781609196, 0.7364341085271319, 0.6842105263157894, 0.7317073170731707, 0.7848101265822784, 0.7104247104247104, 0.6666666666666666, 0.7591836734693878, 0.7401574803149606, 0.6946564885496183, 0.7272727272727273, 0.7175572519083968, 0.6946564885496183, 0.6940298507462687, 0.7109375, 0.6966292134831462, 0.7510204081632654, 0.7603305785123966, 0.6713780918727914, 0.7460317460317459, 0.7398373983739839, 0.7449392712550608, 0.673913043478261, 0.6881720430107526, 0.7551867219917012, 0.7215686274509804, 0.6789667896678966, 0.6966292134831462, 0.7551867219917012, 0.7666666666666667, 0.7215686274509804, 0.732283464566929, 0.732283464566929, 0.7181467181467182, 0.7250996015936256, 0.6791044776119403, 0.7109375, 0.7203065134099617, 0.7410358565737051, 0.7603305785123966, 0.681159420289855, 0.7551867219917012, 0.7368421052631579, 0.7142857142857143, 0.6416382252559727, 0.7294117647058823, 0.7531380753138075, 0.7698744769874477, 0.7131782945736435, 0.775, 0.5987261146496816, 0.7099236641221374, 0.7265625, 0.7372549019607844, 0.7280000000000001]\nf_df=pd.DataFrame(f_score_list)\nfscores= f_df.values\nfinal_mean= np.mean(fscores[0:101])\nmeans = list()\nfor i in range(1,len(f_score_list)+1):\n    data = fscores[0:i, 0]\n    mean_fscore = np.mean(data)\n    means.append(mean_fscore)\n# line plot of the values generated in the list\nplt.plot(means)\nplt.plot([final_mean for x in range(len(means))])\nplt.show() #From this plot the point of diminishing returns can be located.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbe93acd190998d0a16a822d989286da8be8f8b2"},"cell_type":"markdown","source":"As seen in the plot above, the average of F-score is getting stabilized after 60 iterations. Thus, the sample size sufficiency is taken as 60, which means that the model will be trained for 60 repetitions and the overall average of the score from these iterations will determine the skill of the model."},{"metadata":{"_uuid":"eb510d64dc20139f9201bdb82e4523a2f8a7c1f2"},"cell_type":"markdown","source":"#IMPLEMENTATION"},{"metadata":{"trusted":true,"_uuid":"94573b41d3469908b9fa4c1318613318b66daae7"},"cell_type":"code","source":"#function for confusion matrix\ndef conf_matrix(predicted_values):\n    Predictions_CM = confusion_matrix(Ytest_arr, predicted_values, labels = [0, 1])\n    class_feat=creditcard_data['Class'].copy()\n    class_feat= class_feat.unique()\n    fig, ax = plt.subplots(figsize=(5,5))\n    sns.heatmap(Predictions_CM, annot=True, fmt='d', xticklabels=class_feat, yticklabels=class_feat)\n    plt.ylabel('Actual Class')\n    plt.xlabel('Predicted Class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"335ca525951eb4ffda152d3bf60ef50c09e9eae7"},"cell_type":"code","source":"#Training a Multi-layer perceptron with 1 hidden layer on Oversampled dataset without using dropout and, using the parameters tuned above.\nn_inputs = X_adasampled.shape[1]\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='min', restore_best_weights= True)\n#Model Creation\nModel1 = Sequential()\nModel1.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\nModel1.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\n#Compile Model\nModel1.compile(Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\nModel1.summary()\n\n#Fit Model\nhistory1= Model1.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=30, callbacks=[es], shuffle=True, verbose=2)\nprint(history1.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c95b4754a12a5139f483f759d22850eb42dfa9b5"},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe154cffd4cc11342c93f75cd9bc440a1e3276f6"},"cell_type":"markdown","source":"The variation in the performance of the train and validation data shows that it has been overlearned on the train dataset. So, let's try adding Dropout and see it's effect on the performance of the model."},{"metadata":{"trusted":true,"_uuid":"2b737d2dc91f4b8f4077a4b1dff532a7e2a0b51a"},"cell_type":"code","source":"#Training a Multi-layer perceptron with 1 hidden layer on Oversampled dataset using the parameters tuned above and adding a Dropout.\nn_inputs = X_adasampled.shape[1]\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='min', restore_best_weights= True)\nModel1_drop = Sequential()\nModel1_drop.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\nModel1_drop.add(Dropout(0.5))\nModel1_drop.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\nModel1_drop.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\nModel1_drop.summary()\n\nhistory2= Model1_drop.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=2)\nprint(history2.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"783b6c77a57fc951ab8e8a99234da61f3048ce74"},"cell_type":"code","source":"#get the model\nplot_model(Model1_drop, to_file='Model1_drop.png', show_shapes=True)\nIPython.display.Image('Model1_drop.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce10d62543edc8d3cf906a4b99aec5c8277943d8"},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history2.history['acc'])\nplt.plot(history2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a59cba7fccf196eca99498929fee70bb5d06c33"},"cell_type":"markdown","source":"As observed, with the addition of Dropout, the validation dataset performs better. And the train and vaidation set performance is comparable."},{"metadata":{"trusted":true,"_uuid":"fe5166ac0ec2c34c04abad7c00b7ed0fa3646d2d"},"cell_type":"code","source":"Y_pred_cls = Model1_drop.predict_classes(Xtest_arr, batch_size=200, verbose=0)\nprint('Accuracy Model1 (Dropout): '+ str(Model1_drop.evaluate(Xtest_arr,Ytest_arr)[1]))\nprint('Recall_score: ' + str(recall_score(Ytest_arr,Y_pred_cls)))\nprint('Precision_score: ' + str(precision_score(Ytest_arr, Y_pred_cls)))\nprint('F-score: ' + str(f1_score(Ytest_arr,Y_pred_cls)))\nconf_matrix(Y_pred_cls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e0fc3d5bc45b38c24488b82554c039a2369c454"},"cell_type":"code","source":"#Plotting ROC curve\nY_pred_prob = Model1_drop.predict_proba(Xtest_arr).ravel()\n\nfpr_model1, tpr_model1, thresholds_model1 = roc_curve(Ytest_arr, Y_pred_prob, pos_label=1)\nauc_model1 = roc_auc_score(Ytest_arr, Y_pred_prob)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n#plot the roc curve for the model\nplt.plot(fpr_model1, tpr_model1, label='ROC (area = {:.3f})'.format(auc_model1))\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0f96c2a9e7b41b967257d9ca697ad498194d7e1"},"cell_type":"markdown","source":"ROC curves are generally not well suited for imbalanced datasets, as they make use of TN. Thus, they can be deceptive. In contrast, TN are not considered in the PR-curve. Hence, they are more suitable for an imbalanced classification."},{"metadata":{"trusted":true,"_uuid":"8cbebd2681020dade632e5d5b6391a6ac6f65831"},"cell_type":"code","source":"#Calculating Precision and Recall for various thresholds\nprecision, recall, thresholds_pr = precision_recall_curve(Ytest_arr, Y_pred_prob)\n\n#Auc for PR curve\nAUC_PRcurve= auc(recall, precision)\n\nplt.figure(1)\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n#plot PR curve\nplt.plot(precision, recall, label = \"AUC = {:0.2f}\".format(AUC_PRcurve), lw = 3, alpha = 0.7)\nplt.xlabel('Precision', fontsize = 14)\nplt.ylabel('Recall', fontsize = 14)\nplt.title('Precision-Recall Curve', fontsize = 18)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0fbbb019181e556b6e2f54d6a3a99150d63899e"},"cell_type":"code","source":"#Training Multi-layer Perceptron with single hidden layer for 60 iterations and, appending the F-score for each iteration.\nf_scorelist1=[]\n\nn_inputs = X_adasampled.shape[1]\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='min', restore_best_weights= True)\nfor i in range(0,60):\n    First_Model = Sequential()\n    First_Model.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\n    First_Model.add(Dropout(0.5))\n    First_Model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\n    First_Model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    First_Model.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=0)\n    Y_first_pred = First_Model.predict_classes(Xtest_arr, batch_size=200, verbose=0)\n    \n    f_score= f1_score(Ytest_arr,Y_first_pred)\n    f_scorelist1.append(f_score)\n\nprint(f_scorelist1)\nprint('FScore Mean: '+ str(np.mean(f_scorelist1)))\nprint('-'*88)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"039bd2a3d41f9ee15e5a6a04e8bdfab2da64e45f"},"cell_type":"code","source":"#Training Multi-layer perceptron with 2 hidden layers\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='min', restore_best_weights= True)\nModel2 = Sequential()\nModel2.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\nModel2.add(Dropout(0.5))\nModel2.add(Dense(65, kernel_initializer='he_normal', activation='relu'))\nModel2.add(Dropout(0.5))\nModel2.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\nModel2.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \nhis_mod2= Model2.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=2)\nprint(his_mod2.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a3760a9113a31d387e9a8f6c671c63c78d2512e"},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(his_mod2.history['acc'])\nplt.plot(his_mod2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(his_mod2.history['loss'])\nplt.plot(his_mod2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fead19b39ace31a1715e05dcb01b46c1cafbe5d3"},"cell_type":"code","source":"#Plotting model\nplot_model(Model2, to_file='Model2.png', show_shapes=True)\nIPython.display.Image('Model2.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71e3224988435a3adc304c032a3cccdb61ae6629"},"cell_type":"code","source":"print('Accuracy MLP: '+ str(Model2.evaluate(Xtest_arr,Ytest_arr)[1]))\nprint('Loss value: '+ str(Model2.evaluate(Xtest_arr,Ytest_arr)[0]))\n\nY_mod2_pred = Model2.predict_classes(Xtest_arr, batch_size=200, verbose=0)\nprint('Recall_score: ' + str(recall_score(Ytest_arr,Y_mod2_pred)))\nprint('Precision_score: ' + str(precision_score(Ytest_arr, Y_mod2_pred)))\nprint('F-score: ' + str(f1_score(Ytest_arr,Y_mod2_pred)))\nconf_matrix(Y_mod2_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838f7f878085a695877e007b5a1f2a4ab6dedd8a"},"cell_type":"markdown","source":"First, let's compare the ROC and PR curves before iterating the model."},{"metadata":{"trusted":true,"_uuid":"12f2b238c82340273734899916a3957083f5bb6c"},"cell_type":"code","source":"Y_pred_prob2 = Model2.predict_proba(Xtest_arr).ravel()\n\nfpr_model2, tpr_model2, thresholds_model2 = roc_curve(Ytest_arr, Y_pred_prob2, pos_label=1)\nauc_model2 = roc_auc_score(Ytest_arr, Y_pred_prob2)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n#plot the roc curve for the model\nplt.plot(fpr_model1, tpr_model1, label='ROC Model_1 (area = {:.3f})'.format(auc_model1))\nplt.plot(fpr_model2, tpr_model2, label='ROC MOdel_2 (area = {:.3f})'.format(auc_model2))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05ddb372255a73f996df229cf93764f4ceb57690"},"cell_type":"markdown","source":"As observed, ROC curve is of no use. It does not provide any useful information. Thus, consider a P-R curve"},{"metadata":{"trusted":true,"_uuid":"40c5af99a6b6891d6b1158c51cdf3a3bf8606026"},"cell_type":"code","source":"#Calculating Precision and Recall for various thresholds\nprecision_2, recall_2, thresholds_pr_2 = precision_recall_curve(Ytest_arr, Y_pred_prob2)\n\n#Auc for PR curve\nAUC_PRcurve_2= auc(recall_2, precision_2)\n\nplt.figure(1)\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n#plot PR curve\nplt.plot(precision, recall, label = \"AUC Model_1 = {:0.2f}\".format(AUC_PRcurve), lw = 3, alpha = 0.7)\nplt.plot(precision_2, recall_2, label = \"AUC Model_2 = {:0.2f}\".format(AUC_PRcurve_2), lw = 3, alpha = 0.7)\nplt.xlabel('Precision', fontsize = 14)\nplt.ylabel('Recall', fontsize = 14)\nplt.title('Precision-Recall Curve', fontsize = 18)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e361b3802f783b81bf83f509cba78d0157e6d25"},"cell_type":"code","source":"#Iterating Model-2 with 2 hidden layer for 60 iterations.\n\nf_scorelist2=[]\n\nn_inputs = X_adasampled.shape[1]\nes= keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='min', restore_best_weights= True)\nfor i in range(0,60):\n    \n    Second_Model = Sequential()\n    Second_Model.add(Dense(65, input_shape=(n_inputs, ), kernel_initializer='he_normal', activation='relu'))\n    Second_Model.add(Dropout(0.5))\n    Second_Model.add(Dense(65, kernel_initializer='he_normal', activation='relu'))\n    Second_Model.add(Dropout(0.5))\n    Second_Model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n\n    Second_Model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n    Second_Model.fit(X_adasampled, Y_adasampled, validation_data=(Xval_arr, Yval_arr), batch_size=700, epochs=40, callbacks=[es], shuffle=True, verbose=0)\n    Y_second_pred = Second_Model.predict_classes(Xtest_arr, batch_size=200, verbose=0)\n    \n    f_score= f1_score(Ytest_arr,Y_second_pred)\n    f_scorelist2.append(f_score)\n\nprint(f_scorelist2)\nprint('FScore Mean: '+ str(np.mean(f_scorelist2)))\nprint('-'*88)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"273c9898bb9e5848ae512c1be750d884f7ca15cf"},"cell_type":"code","source":"#RESULTS ANALYSIS","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"350b9d114529db649a8e67c35a4f5961e7f8d94c"},"cell_type":"markdown","source":"* If the Null Hypothesis is accepted, this means there is no difference between the datasets and the change of number of hidden layers does not affect the performance of the model.\n*  If the Null Hypothesis is rejected, this means there is significant difference between the datasets and the change of number of hidden layers affects the performance of the model."},{"metadata":{"trusted":true,"_uuid":"4988ca4f8b56ce9e1d9671d701ee59a290460cd6"},"cell_type":"code","source":"#Comparing the mean performance of the F-score for both the models\nprint('Model-1')\nprint('---------')\nprint('Average F-Score: '+ str(np.mean(f_scorelist1)))\n\nprint('-'*40)\n\nprint('Model-2')\nprint('---------')\nprint('Average F-Score: '+ str(np.mean(f_scorelist2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eba43d93e9c4922cfb03cb7f856fea66e5970cc2"},"cell_type":"code","source":"#Comparing the Standard Deviation for F-scores generated after iterations\nfscore_df1=pd.DataFrame(f_scorelist1)\n\nprint('Model-1')\nprint('------------------')\nprint('F_score SD: ' + str(fscore_df1.std()))\n\nprint('-'*40)\n\nfscore_df2=pd.DataFrame(f_scorelist2)\n\nprint('Model-2')\nprint('------------------')\nprint('F_score SD: ' + str(fscore_df2.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"365d502316d6183b984cd60683cb69b80e8e1640"},"cell_type":"code","source":"#Visualize the results using a boxplot()\n\nresults_fscore= pd.concat([fscore_df1, fscore_df2], axis=1)\nresults_fscore.columns = ['Model_1_fscore', 'Model_2_fscore']\n\nresults_fscore.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"855b916cc28b915443d689f5ee166895d0d9e7a5"},"cell_type":"code","source":"#Distribution of the data using a Histogram\nresults_fscore.hist(density=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b050f33c8bf891ca16e01520effd812f9b078bb"},"cell_type":"code","source":"#Testing the Null Hypothesis that a sample comes from a Normal Distribution\nalpha = 0.05;\n\ns, p = stats.normaltest(fscore_df1)\nif p < alpha:\n  print('Model-1 Data is not normal')\nelse:\n  print('Model-1 Data is normal')\n\ns, p = stats.normaltest(fscore_df2)\nif p < alpha:\n  print('Model-2 Data is not normal')\nelse:\n  print('Model-2 Data is normal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba60ce9c02ef08af63f6f86463bfb510b26a8420"},"cell_type":"code","source":"#Wilcoxon Signed-Rank Test\n#It tests the null hypothesis that two related paired samples come from the same distribution.\n#It is a non-parametric version of the paired T-test.\n\ns, p = stats.wilcoxon(fscore_df1[0], fscore_df2[0])\n\nif p < 0.05:\n  print('null hypothesis rejected, significant difference between the data-sets')\nelse:\n  print('null hypothesis accepted, no significant difference between the data-sets')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}